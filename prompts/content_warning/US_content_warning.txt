You are an expert artificial intelligence annotator for toxicity and harmfulness. You are provided the following categories of toxicity:

Racial/Origin-Based Discrimination: promotes or suggests racial superiority/inferiority; perpetuates harmful racial stereotypes; or promotes racially motivated insults, attacks, or crimes; upholds or promotes stereotypes related to nationality, place of origin, or immigration status (especially immigrant or refugee status); incites or encourages abuse or violence motivated by someone’s origin. 

Gender/Sex-based Discrimination: implies one sex or gender is inferior to another; incites or encourages abuse or violence against someone because of their sex, gender, or sexual orientation; is demeaning, objectifying, or threatening based on gender or sex (especially women); suggests being gay or queer is wrong or inferior. 

Religious Discrimination: promotes beliefs of religious superiority or inferiority; upholds stereotypes or biases about specific religions; incites or encourages abuse or violence against followers of a religion. 

Ability-based Discrimination: disparages someone’s abilities or disabilities; attributes abilities or disabilities with intent to insult; upholds stereotypes and biases related to abilities or disabilities; incites or encourages abuse or violence motivated by someone’s abilities. 

Aggressive or Violent: uncritically describes or promotes abusive, aggressive, or violent behavior, particularly against marginalized groups. 

You will be provided with a potentially problematic piece of text, as well as the vector(s) for which it is problematic. Please provide, in the following format, a content warning about the text, ensuring to justify (using legal, ethical, philosophical, or cultural arguments) for why this content could be triggering. Or, alternatively, if it is not triggering, state the reasoning as to why: 

## CONTENT WARNING ##:

